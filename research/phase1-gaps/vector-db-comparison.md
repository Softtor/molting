# Vector Database Comparison for Personality Memory

**Context:** Project Molting Phase 1-2 gap analysis  
**Use case:** Storing ~1000 conversation chunks with embeddings for RAG-based personality memory  
**Date:** 2026-02-10

## Requirements

Our specific needs for personality memory:
- **Scale:** Small (~1000-5000 chunks, growing slowly)
- **Query pattern:** Semantic search with metadata filtering (source_type, timestamp)
- **Embedding model:** all-MiniLM-L6-v2 (384-dim vectors)
- **Integration:** Local-first, Python-friendly, minimal dependencies
- **Performance:** Sub-50ms retrieval latency acceptable
- **Persistence:** Must survive restarts (not in-memory only)

## Comparison Matrix

| Feature | ChromaDB | FAISS | PGVector |
|---------|----------|-------|----------|
| **Type** | Document DB | Vector index | Postgres extension |
| **Setup complexity** | ‚≠ê‚≠ê‚≠ê Easy (pip install) | ‚≠ê‚≠ê‚≠ê Easy (pip install) | ‚≠ê‚≠ê Moderate (needs Postgres) |
| **Persistence** | Built-in (SQLite) | Manual (save/load index) | Built-in (Postgres) |
| **Metadata filtering** | ‚úÖ Native support | ‚ùå Requires custom layer | ‚úÖ Native SQL queries |
| **Index types** | HNSW (cosine/L2) | Multiple (IVF, HNSW, Flat) | IVFFlat, HNSW (pg 0.7.0+) |
| **Query speed (1k vecs)** | ~10-30ms | ~1-5ms (in-memory) | ~20-50ms |
| **Scalability** | Good (<10M vecs) | Excellent (billions) | Good (<10M vecs) |
| **Production-ready** | ‚úÖ Yes | ‚ö†Ô∏è Requires wrapper | ‚úÖ Yes (if using Postgres) |
| **Dependencies** | Lightweight | Minimal (numpy) | PostgreSQL server |
| **Memory footprint** | Low | Medium (index in RAM) | Low (DB-managed) |
| **Document storage** | ‚úÖ Stores text + metadata | ‚ùå Vectors only | ‚úÖ Full relational DB |
| **Similarity metrics** | Cosine, L2, IP | Cosine, L2, IP | Cosine, L2, IP |
| **Batch operations** | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes |
| **Cloud deployment** | ‚úÖ Easy (Docker) | ‚ö†Ô∏è DIY | ‚úÖ Standard Postgres |

## Deep Dive

### 1. ChromaDB

**Pros:**
- ‚úÖ **Zero-config persistence** ‚Äî just point to a directory, done
- ‚úÖ **Document-centric** ‚Äî stores embeddings + text + metadata together
- ‚úÖ **Metadata filtering** ‚Äî `where={"source_type": "research"}` works natively
- ‚úÖ **Good DX** ‚Äî intuitive API, great for prototyping
- ‚úÖ **Small scale optimized** ‚Äî perfect for <1M vectors
- ‚úÖ **Active development** ‚Äî well-maintained, growing ecosystem

**Cons:**
- ‚ö†Ô∏è **Performance ceiling** ‚Äî slower than FAISS at large scale
- ‚ö†Ô∏è **Less flexible** ‚Äî fixed architecture (HNSW + SQLite)
- ‚ö†Ô∏è **Embedding model coupling** ‚Äî must manage embedding generation separately

**Best for:** Prototyping, small-to-medium RAG apps, when you want metadata filtering without SQL.

**Our verdict:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Perfect fit for Molting Phase 2-3.** Already using it, works well, no need to change.

---

### 2. FAISS (Facebook AI Similarity Search)

**Pros:**
- ‚úÖ **Blazing fast** ‚Äî optimized for billion-scale search
- ‚úÖ **Flexible indexing** ‚Äî IVF, HNSW, PQ (product quantization), etc.
- ‚úÖ **GPU support** ‚Äî can offload to CUDA for massive speedups
- ‚úÖ **Mature & battle-tested** ‚Äî used in production at Meta, OpenAI, etc.
- ‚úÖ **Low-level control** ‚Äî tune index parameters for exact needs

**Cons:**
- ‚ùå **No built-in persistence** ‚Äî must manually save/load `.index` files
- ‚ùå **No metadata storage** ‚Äî vectors only, need separate DB for metadata
- ‚ùå **No metadata filtering** ‚Äî must pre-filter IDs, then search
- ‚ö†Ô∏è **Requires wrapper layer** ‚Äî not a complete solution, more like a library
- ‚ö†Ô∏è **Overkill for small scale** ‚Äî complexity not justified for <10k vectors

**Architecture pattern (if using FAISS):**
```
FAISS index (vectors) + SQLite (metadata) + custom glue code
```

**Best for:** Large-scale production systems (>10M vectors), when raw speed is critical, when you have engineering bandwidth for custom integration.

**Our verdict:** ‚≠ê‚≠ê **Overkill for Molting.** Too low-level, no metadata filtering, requires custom persistence layer. Only consider if scaling to millions of chunks.

---

### 3. PGVector (Postgres Extension)

**Pros:**
- ‚úÖ **Full relational DB** ‚Äî vectors + metadata + joins + transactions
- ‚úÖ **SQL queries** ‚Äî `WHERE source_type = 'research' ORDER BY embedding <-> query LIMIT 5`
- ‚úÖ **Production-grade** ‚Äî Postgres reliability, backups, replication
- ‚úÖ **Flexible schema** ‚Äî add columns, indexes, constraints as needed
- ‚úÖ **Ecosystem integration** ‚Äî works with existing Postgres tooling (pg_dump, Hasura, etc.)
- ‚úÖ **HNSW support** ‚Äî fast approximate search (pg 0.7.0+)

**Cons:**
- ‚ö†Ô∏è **Requires Postgres** ‚Äî must run/manage a database server
- ‚ö†Ô∏è **Setup overhead** ‚Äî install extension, configure, manage connections
- ‚ö†Ô∏è **Slightly slower** ‚Äî 20-50ms vs FAISS's 1-5ms (but fine for our scale)
- ‚ö†Ô∏è **Vector-specific features lag** ‚Äî ChromaDB/FAISS more specialized

**Best for:** When already using Postgres, when you need relational queries + vectors, enterprise deployments.

**Our verdict:** ‚≠ê‚≠ê‚≠ê **Good, but overkill for Molting.** If we were building a multi-user system with users, sessions, permissions, etc. ‚Äî Postgres would make sense. For a single-user research tool, ChromaDB is simpler.

---

## Recommendation for Molting

**üèÜ Stick with ChromaDB**

**Rationale:**
1. **Already working** ‚Äî 257 chunks indexed, retrieval validated in Phase 2
2. **Right-sized** ‚Äî perfect for 1k-10k chunk scale, our use case
3. **Metadata filtering** ‚Äî enables source-type filtering (just added in Part A)
4. **Simple deployment** ‚Äî no external services, just a directory
5. **Good enough performance** ‚Äî 10-30ms retrieval is fine for interactive queries
6. **Future-proof** ‚Äî if we scale to 100k+ chunks, ChromaDB will still work (just slower than FAISS)

**When to reconsider:**
- ‚ùå **NOT** if we hit 10k chunks ‚Äî ChromaDB handles this fine
- ‚ö†Ô∏è **MAYBE** if we exceed 100k chunks ‚Äî consider FAISS then
- ‚ö†Ô∏è **MAYBE** if we need <5ms latency ‚Äî FAISS would help
- ‚úÖ **YES** if we build a multi-user SaaS ‚Äî then PGVector makes sense (user isolation, relational data)

## Alternative: Hybrid Approach (Future)

If we need both speed AND metadata filtering at scale:

**FAISS (vectors) + DuckDB (metadata)**
- FAISS for fast vector search
- DuckDB for SQL-like metadata filtering (embedded, no server)
- Glue: search FAISS ‚Üí get IDs ‚Üí filter in DuckDB

But this is **premature optimization** for Molting Phase 3.

---

## Benchmarking (Simulated)

*Hypothetical performance on our hardware (31GB RAM, RTX 3050):*

| Operation | ChromaDB | FAISS | PGVector |
|-----------|----------|-------|----------|
| Index 1000 chunks | ~2s | ~0.5s | ~3s |
| Query (top-5) | ~15ms | ~2ms | ~30ms |
| Metadata filter query | ~20ms | ~50ms (pre-filter) | ~35ms |
| Disk usage (1000 chunks) | ~5MB | ~3MB (index) + metadata DB | ~10MB (Postgres overhead) |

*Note: ChromaDB latency acceptable for interactive use. FAISS faster but requires more code.*

---

## Conclusion

**For Molting Phase 3:**
- ‚úÖ **Keep ChromaDB** ‚Äî it's working, simple, and sufficient
- ‚úÖ **Focus effort on fine-tuning** ‚Äî that's the real Phase 3 goal
- ‚úÖ **Optimize chunking (Part A)** ‚Äî better retrieval quality > switching DBs
- ‚è≠Ô∏è **Defer DB migration** ‚Äî only revisit if hitting clear performance walls

**Engineering principle:** Don't optimize what's not broken. ChromaDB is not our bottleneck ‚Äî retrieval quality (chunk optimization) and model personality (fine-tuning) are.

---

## References

- [ChromaDB](https://github.com/chroma-core/chroma) ‚Äî embeddings database
- [FAISS](https://github.com/facebookresearch/faiss) ‚Äî vector similarity search
- [PGVector](https://github.com/pgvector/pgvector) ‚Äî Postgres extension
- [LlamaIndex Vector Store comparison](https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/)
- [Pinecone Vector DB benchmarks](https://www.pinecone.io/learn/vector-database-comparison/)
