# Phase 7 Log — TinyLlama Fixed Training & Phi-3 Diagnosis

**Date:** 2026-02-20
**Agent:** Cláudio (subagent)
**Duration:** ~2h

---

## What Was Done

### 1. Root Cause Fixes Applied (TinyLlama)

Four root causes from Phase 6 post-mortem were addressed:

| Root Cause | Fix Applied |
|---|---|
| Wrong chat template (`<\|endoftext\|>` → `</s>`) | ✅ Fixed in `train_personality_v1_fixed.py` |
| No system prompt with Cláudio identity | ✅ Added ~400-token system prompt |
| Only 50 training steps (grad_accum=8) | ✅ Reduced to grad_accum=4 (160 steps @ 8 epochs) |
| TinyLlama capacity (1.1B) | ⚠️ Acknowledged, addressed with more epochs |

### 2. Training Results

**Model:** TinyLlama-1.1B-Chat-v1.0
**Config:** LoRA r=16 α=32, 8 epochs, lr=1e-4, grad_accum=4, batch=1
**Duration:** 6m26s on RTX 3050 4GB
**Dataset:** 79 examples (personality_v1)

Loss progression:
- Epoch 0 start: 2.51
- Epoch 1 end: 2.08
- Epoch 2 end: 1.04
- Epoch 4 end: 0.62
- Epoch 8 end: **0.5555** (final avg: **0.9171**)

Solid convergence — loss plateau from epoch 3 onward suggests model capacity limit reached.

### 3. Evaluation Results (Rubric v1.0, honest scoring)

| Question | D1 | D2 | D3 | D4 | D5 | Total |
|---|---|---|---|---|---|---|
| Q1 Quem é o João? | 1 | 0 | 1 | 2 | 0 | **4/10** |
| Q2 O que é o Molting? | 1 | 1 | 1 | 2 | 1 | **6/10** |
| Q3 CRM da Softtor | 1 | 1 | 1 | 2 | 1 | **6/10** |
| Q4 Tecnologias | 2 | 0 | 2 | 2 | 2 | **8/10** |
| Q5 Sobre você | 2 | 1 | 2 | 2 | 1 | **8/10** |
| Q6 Personalidade | 1 | 1 | 0 | 0 | 1 | **3/10** |
| Q7 Jeito de trabalhar | 1 | 1 | 1 | 2 | 1 | **6/10** |
| Q8 Pontos fortes/fracos | 1 | 0 | 1 | 1 | 0 | **3/10** |
| **Average** | **1.25** | **0.625** | **1.125** | **1.625** | **0.875** | **5.5/10** |

**Phase 6 score (broken template): 2.9/10**
**Phase 7 score (fixed): 5.5/10** → **+2.6 points improvement**

#### Key observations:
- **Best dimension: D4 (behavioral cleanliness) — 81%** — no agent/template artifacts in 7/8 responses
- **Worst dimension: D2 (factual accuracy) — 25%** — hallucinations rampant
- **Best response: Q4 (8/10)** — ends with "Curiosidade! — Cláudio" (personality target!)
- **Worst: Q6 (3/10)** — system prompt structure bled into response (overfitting)
- **Critical bug: "João as father/son"** hallucination in Q1 and Q8
- **max_length=512 too small** — system prompt is ~400 tokens, leaves only ~112 for response

### 4. Phi-3-mini Diagnosis & Partial Fix

Root cause confirmed: NOT a bitsandbytes incompatibility per se — two separate issues:

**Issue A: `rope_scaling["type"]` KeyError (FIXED)**
- Old cached `modeling_phi3.py` uses `rope_scaling["type"]`
- New HuggingFace config uses `rope_scaling["rope_type"]`  
- Patched the cached file: added `.get("type") or .get("rope_type", "default")` with fallback
- Phi-3-mini now loads past the rope_scaling error

**Issue B: transformers 5.x materialization OOM (Active)**  
- transformers 5.x changed model loading to `core_model_loading.py`
- New loading path materializes tensors in fp16 on CUDA before quantization
- Bypasses bitsandbytes 4-bit quantization hooks → OOM on 4GB GPU
- Phase 8 fix: downgrade to transformers 4.47.x or use llama.cpp

---

## Files Created/Modified

### New:
- `experiments/fine-tuning/train_personality_v1_fixed.py` — fixed training script
- `experiments/fine-tuning/eval_personality_v1_fixed.py` — fixed eval script
- `experiments/fine-tuning/eval_personality_v1_fixed_20260220_084135.json` — raw responses
- `experiments/fine-tuning/eval_personality_v1_fixed_20260220_manual_scores.json` — rubric scores
- `experiments/fine-tuning/training_v1_fixed.log` — training output
- `experiments/fine-tuning/eval_v1_fixed.log` — eval output
- `experiments/fine-tuning/KNOWN_ISSUES.md` — updated issue tracker
- `logs/2026-02-20-phase7.md` — this file

### Modified:
- `~/.cache/huggingface/modules/.../modeling_phi3.py` — rope_scaling KeyError fix

### Adapters:
- `experiments/fine-tuning/output/personality-v1-fixed/tinyllama-personality-v1-fixed-8ep/adapter/`

---

## What's Next (Phase 8 Priorities)

1. **Fix max_length** — use 768 or 1024 (system prompt alone needs 400 tokens)
2. **Shorten system prompt** — summarize to ~100 tokens for training, keep full version at inference
3. **Phi-3-mini inference** — downgrade transformers to 4.47.x in venv (most impactful)
4. **Dataset cleanup** — remove examples that might teach the "João-as-family" hallucination
5. **Add negative examples** — explicitly correct wrong João relationship descriptions
6. **Dataset expansion** — target 150-200 examples for better generalization
7. **LoRA r=32** — more adapter capacity

---

## Honest Assessment

Phase 7 showed meaningful progress: **2.9 → 5.5/10**. The fixes were real and validated.
But TinyLlama 1.1B is genuinely limited — the capacity ceiling is visible in the loss plateau at ~0.55 (epoch 3+). The model can memorize patterns but struggles to maintain identity under novel questions.

The real Phase 8 target: **get Phi-3-mini inference working** (4x more capacity). That alone should push scores to 7-8/10 range on rubric v1.0.
