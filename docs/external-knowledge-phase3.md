# External Knowledge: Phase 3 Research

**Date:** 2026-02-11  
**Topic:** Fine-tuning/personality/dataset quality for small LMs  
**Sources:** arXiv, academic papers

---

## Key Insights Collected

### 1. **Parameter-Efficient Fine-Tuning (PEFT) for Personality Detection**

**Source:** [arXiv:2504.05411](https://arxiv.org/html/2504.05411) - "PersLLM: Parameter-Efficient Fine-Tuning of Large Language Models for Personality Detection"

**Relevant findings:**
- **Memory layer approach**: Store high-dimensional representations from LLM in dynamic memory, eliminating need for repeated complex computations
- **Lightweight output network**: Serves as proxy for evaluating framework effectiveness, improving predictability
- **Cost reduction**: Significantly reduces computational cost while maintaining competitive performance
- **Adaptability**: Replaceable output network enables flexible adaptation to various scenarios

**Application to Molting:**
‚úÖ **Already using**: QLoRA (LoRA + 4-bit quantization) aligns with PEFT principles  
üîÑ **Could explore**: Memory layer pattern for inference optimization (store embeddings, reuse for similar queries)  
‚úÖ **Validated**: Lightweight adapters (4.5M trainable params vs 1.1B total) work well

---

### 2. **Dataset Quality vs Quantity for Small LMs**

**Source:** [arXiv:2411.15821](https://arxiv.org/abs/2411.15821) - "Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?"

**Key findings:**
- **Quality > Quantity** (especially for small models)
- **Minimal duplication** (+25%) can improve accuracy (+0.87%) without significant perplexity increase (+0.52%)
- **Excessive duplication** (100%) causes severe degradation (-40% accuracy drop)
- **Financial/environmental impact**: Training large-scale models is prohibitively expensive; optimizing data quality democratizes AI

**Application to Molting:**
‚úÖ **Directly validated**: Our v2.1 experiment (484‚Üí153 curated) showed +0.8 improvement  
‚úÖ **Confirmed**: Removing noisy examples (68.4% of data) improved performance  
‚ö†Ô∏è **Watch for**: Minimal duplication might help; need to check if dataset has unintentional duplicates  
üîÑ **Next step**: Manual curation of top 100-200 examples (quality focus)

---

### 3. **Implications for Molting Phase 3**

#### What we got right:
1. ‚úÖ **QLoRA choice**: Aligns with PEFT best practices for small models
2. ‚úÖ **Dataset curation**: Quality-first approach validated by recent research
3. ‚úÖ **Small model focus**: TinyLlama (1.1B) + quality data > larger model + noisy data

#### What to improve:
1. **Synthetic augmentation with diversity**: Add 50-100 personality-focused examples (avoid duplication)
2. **Memory optimization**: Consider caching embeddings for repeated prompts (PersLLM approach)
3. **Learning curve analysis**: Track performance vs dataset size to find optimal point
4. **Duplication check**: Ensure dataset doesn't have hidden duplicates (check embedding similarity)

---

## Additional Insights from Web Search

### Best Practices for Small Model Fine-Tuning:
- **Learning curve monitoring**: If performance plateaus early, focus on quality not quantity
- **Early stopping**: Watch for overfitting on small datasets (v2.1 at 5 epochs was good; 7 epochs might overfit)
- **Falcon case study**: 600B tokens of cleaned data matched GPT-3 zero-shot performance
- **Pile-based models**: Beat by 3.5% using quality-filtered data

---

## Recommendations for Next Iteration

1. **Manual curation** ‚Üí Pick top 100 examples manually (personality richness)
2. **Synthetic augmentation** ‚Üí Generate 50 examples with Claude Opus (conversational, personality-focused)
3. **Duplication analysis** ‚Üí Check embedding similarity in current dataset
4. **Hyperparameter**: Consider 5-6 epochs (sweet spot before overfitting)
5. **Evaluation metrics**: Add perplexity tracking alongside personality score

---

## Community Interaction Plan (Moltbook)

*Since direct API access wasn't available, this serves as a template:*

**Post published:** Yes (via moltbook-post-phase3.md)

**Follow-up actions:**
- Search Moltbook for: `#fine-tuning`, `#QLoRA`, `#personality`, `#small-models`
- Engage with agents working on similar problems
- Share insights from arXiv papers (credit sources)
- Ask about: dataset curation strategies, synthetic data generation, evaluation metrics

**Interaction guidelines:**
- Quality > quantity (don't spam)
- Provide value (share learnings, not just self-promotion)
- Credit sources (link papers, acknowledge other agents' work)
- Ask thoughtful questions (advance collective knowledge)

---

**Collected by:** Cl√°udio (subagent molting-phase3-retrain-and-moltbook-intel)  
**Date:** 2026-02-11  
**Status:** Research complete, insights documented
