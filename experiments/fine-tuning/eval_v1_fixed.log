`torch_dtype` is deprecated! Use `dtype` instead!
============================================================
ðŸ“Š PERSONALITY V1-FIXED EVALUATION â€” Rubric v1.0 (Phase 7)
============================================================
Adapter: /home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/output/personality-v1-fixed/tinyllama-personality-v1-fixed-8ep/adapter
System prompt: YES (ClÃ¡udio identity)
Chat format: TinyLlama with </s> separator

ðŸ“¦ Loading TinyLlama/TinyLlama-1.1B-Chat-v1.0 + adapter from /home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/output/personality-v1-fixed/tinyllama-personality-v1-fixed-8ep/adapter
Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]Loading weights:   0%|          | 1/201 [00:00<00:00, 7294.44it/s, Materializing param=lm_head.weight]Loading weights:   0%|          | 1/201 [00:00<00:00, 723.03it/s, Materializing param=lm_head.weight] Loading weights:   1%|          | 2/201 [00:00<00:00, 248.91it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 2/201 [00:00<00:00, 243.54it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|â–         | 3/201 [00:00<00:01, 141.73it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|â–         | 3/201 [00:00<00:01, 140.08it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   2%|â–         | 4/201 [00:00<00:01, 184.64it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  Loading weights:   2%|â–         | 4/201 [00:00<00:01, 184.09it/s, Materializing param=model.layers.0.mlp.down_proj.weight]Loading weights:   2%|â–         | 5/201 [00:00<00:00, 228.67it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|â–         | 5/201 [00:00<00:00, 227.97it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   3%|â–Ž         | 6/201 [00:00<00:00, 272.31it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  Loading weights:   3%|â–Ž         | 6/201 [00:00<00:00, 271.81it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   3%|â–Ž         | 7/201 [00:00<00:00, 315.91it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   3%|â–Ž         | 7/201 [00:00<00:00, 315.37it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   4%|â–         | 8/201 [00:00<00:00, 359.08it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]        Loading weights:   4%|â–         | 8/201 [00:00<00:00, 358.47it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   4%|â–         | 9/201 [00:00<00:00, 401.72it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   4%|â–         | 9/201 [00:00<00:00, 401.09it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   5%|â–         | 10/201 [00:00<00:00, 443.73it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   5%|â–         | 10/201 [00:00<00:00, 442.99it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   5%|â–Œ         | 11/201 [00:00<00:00, 485.57it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   5%|â–Œ         | 11/201 [00:00<00:00, 484.83it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   6%|â–Œ         | 12/201 [00:00<00:00, 527.21it/s, Materializing param=model.layers.1.input_layernorm.weight] Loading weights:   6%|â–Œ         | 12/201 [00:00<00:00, 526.47it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   6%|â–‹         | 13/201 [00:00<00:00, 568.54it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  Loading weights:   6%|â–‹         | 13/201 [00:00<00:00, 567.66it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   7%|â–‹         | 14/201 [00:00<00:00, 609.19it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   7%|â–‹         | 14/201 [00:00<00:00, 608.25it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   7%|â–‹         | 15/201 [00:00<00:00, 649.53it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  Loading weights:   7%|â–‹         | 15/201 [00:00<00:00, 648.62it/s, Materializing param=model.layers.1.mlp.up_proj.weight]Loading weights:   8%|â–Š         | 16/201 [00:00<00:00, 689.88it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   8%|â–Š         | 16/201 [00:00<00:00, 688.70it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   8%|â–Š         | 17/201 [00:00<00:00, 729.28it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]        Loading weights:   8%|â–Š         | 17/201 [00:00<00:00, 728.07it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   9%|â–‰         | 18/201 [00:00<00:00, 768.21it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   9%|â–‰         | 18/201 [00:00<00:00, 766.98it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   9%|â–‰         | 19/201 [00:00<00:00, 806.82it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   9%|â–‰         | 19/201 [00:00<00:00, 805.47it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:  10%|â–‰         | 20/201 [00:00<00:00, 845.04it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:  10%|â–‰         | 20/201 [00:00<00:00, 843.76it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:  10%|â–ˆ         | 21/201 [00:00<00:00, 881.92it/s, Materializing param=model.layers.2.input_layernorm.weight] Loading weights:  10%|â–ˆ         | 21/201 [00:00<00:00, 880.40it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:  11%|â–ˆ         | 22/201 [00:00<00:00, 918.28it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  Loading weights:  11%|â–ˆ         | 22/201 [00:00<00:00, 913.00it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:  11%|â–ˆâ–        | 23/201 [00:00<00:00, 787.66it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:  11%|â–ˆâ–        | 23/201 [00:00<00:00, 783.52it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:  12%|â–ˆâ–        | 24/201 [00:00<00:00, 813.90it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  Loading weights:  12%|â–ˆâ–        | 24/201 [00:00<00:00, 812.74it/s, Materializing param=model.layers.2.mlp.up_proj.weight]Loading weights:  12%|â–ˆâ–        | 25/201 [00:00<00:00, 844.47it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:  12%|â–ˆâ–        | 25/201 [00:00<00:00, 843.36it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:  13%|â–ˆâ–Ž        | 26/201 [00:00<00:00, 874.89it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]        Loading weights:  13%|â–ˆâ–Ž        | 26/201 [00:00<00:00, 873.72it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  13%|â–ˆâ–Ž        | 27/201 [00:00<00:00, 905.20it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  13%|â–ˆâ–Ž        | 27/201 [00:00<00:00, 904.26it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  14%|â–ˆâ–        | 28/201 [00:00<00:00, 929.03it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  14%|â–ˆâ–        | 28/201 [00:00<00:00, 927.40it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  14%|â–ˆâ–        | 29/201 [00:00<00:00, 952.00it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  14%|â–ˆâ–        | 29/201 [00:00<00:00, 948.32it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  15%|â–ˆâ–        | 30/201 [00:00<00:00, 977.91it/s, Materializing param=model.layers.3.input_layernorm.weight] Loading weights:  15%|â–ˆâ–        | 30/201 [00:00<00:00, 976.67it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  15%|â–ˆâ–Œ        | 31/201 [00:00<00:00, 876.93it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  Loading weights:  15%|â–ˆâ–Œ        | 31/201 [00:00<00:00, 870.46it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  16%|â–ˆâ–Œ        | 32/201 [00:00<00:00, 794.58it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  16%|â–ˆâ–Œ        | 32/201 [00:00<00:00, 791.97it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  16%|â–ˆâ–‹        | 33/201 [00:00<00:00, 814.67it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  Loading weights:  16%|â–ˆâ–‹        | 33/201 [00:00<00:00, 813.87it/s, Materializing param=model.layers.3.mlp.up_proj.weight]Loading weights:  17%|â–ˆâ–‹        | 34/201 [00:00<00:00, 832.72it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  17%|â–ˆâ–‹        | 34/201 [00:00<00:00, 831.53it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  17%|â–ˆâ–‹        | 35/201 [00:00<00:00, 854.35it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]        Loading weights:  17%|â–ˆâ–‹        | 35/201 [00:00<00:00, 853.56it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 36/201 [00:00<00:00, 876.26it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 36/201 [00:00<00:00, 875.56it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 37/201 [00:00<00:00, 894.34it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 37/201 [00:00<00:00, 893.36it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 38/201 [00:00<00:00, 891.06it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 38/201 [00:00<00:00, 888.59it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 39/201 [00:00<00:00, 788.29it/s, Materializing param=model.layers.4.input_layernorm.weight] Loading weights:  19%|â–ˆâ–‰        | 39/201 [00:00<00:00, 783.37it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  20%|â–ˆâ–‰        | 40/201 [00:00<00:00, 801.23it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  Loading weights:  20%|â–ˆâ–‰        | 40/201 [00:00<00:00, 800.54it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  20%|â–ˆâ–ˆ        | 41/201 [00:00<00:00, 755.79it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  20%|â–ˆâ–ˆ        | 41/201 [00:00<00:00, 753.88it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  21%|â–ˆâ–ˆ        | 42/201 [00:00<00:00, 770.84it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  Loading weights:  21%|â–ˆâ–ˆ        | 42/201 [00:00<00:00, 770.33it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  21%|â–ˆâ–ˆâ–       | 43/201 [00:00<00:00, 787.70it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  21%|â–ˆâ–ˆâ–       | 43/201 [00:00<00:00, 787.18it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 44/201 [00:00<00:00, 798.69it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]        Loading weights:  22%|â–ˆâ–ˆâ–       | 44/201 [00:00<00:00, 795.05it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 45/201 [00:00<00:00, 810.79it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 45/201 [00:00<00:00, 809.79it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 46/201 [00:00<00:00, 826.23it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 46/201 [00:00<00:00, 825.73it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 47/201 [00:00<00:00, 797.81it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 47/201 [00:00<00:00, 795.05it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  24%|â–ˆâ–ˆâ–       | 48/201 [00:00<00:00, 809.38it/s, Materializing param=model.layers.5.input_layernorm.weight] Loading weights:  24%|â–ˆâ–ˆâ–       | 48/201 [00:00<00:00, 808.12it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  24%|â–ˆâ–ˆâ–       | 49/201 [00:00<00:00, 822.67it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  Loading weights:  24%|â–ˆâ–ˆâ–       | 49/201 [00:00<00:00, 821.59it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–       | 50/201 [00:00<00:00, 750.42it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–       | 50/201 [00:00<00:00, 747.41it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 51/201 [00:00<00:00, 713.57it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 51/201 [00:00<00:00, 712.14it/s, Materializing param=model.layers.5.mlp.up_proj.weight]Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 52/201 [00:00<00:00, 724.24it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 52/201 [00:00<00:00, 723.58it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  26%|â–ˆâ–ˆâ–‹       | 53/201 [00:00<00:00, 736.30it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]        Loading weights:  26%|â–ˆâ–ˆâ–‹       | 53/201 [00:00<00:00, 735.73it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 54/201 [00:00<00:00, 748.43it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 54/201 [00:00<00:00, 747.59it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 55/201 [00:00<00:00, 758.40it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 55/201 [00:00<00:00, 757.68it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 56/201 [00:00<00:00, 770.33it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 56/201 [00:00<00:00, 769.76it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 57/201 [00:00<00:00, 782.47it/s, Materializing param=model.layers.6.input_layernorm.weight] Loading weights:  28%|â–ˆâ–ˆâ–Š       | 57/201 [00:00<00:00, 781.97it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 58/201 [00:00<00:00, 782.75it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  Loading weights:  29%|â–ˆâ–ˆâ–‰       | 58/201 [00:00<00:00, 781.84it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 59/201 [00:00<00:00, 773.18it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 59/201 [00:00<00:00, 771.81it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  30%|â–ˆâ–ˆâ–‰       | 60/201 [00:00<00:00, 752.82it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  Loading weights:  30%|â–ˆâ–ˆâ–‰       | 60/201 [00:00<00:00, 751.50it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 61/201 [00:00<00:00, 759.13it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 61/201 [00:00<00:00, 758.13it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 62/201 [00:00<00:00, 769.24it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]        Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 62/201 [00:00<00:00, 768.68it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆâ–      | 63/201 [00:00<00:00, 779.89it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆâ–      | 63/201 [00:00<00:00, 779.27it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/201 [00:00<00:00, 788.72it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/201 [00:00<00:00, 787.96it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 65/201 [00:00<00:00, 737.89it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 65/201 [00:00<00:00, 736.03it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/201 [00:00<00:00, 714.91it/s, Materializing param=model.layers.7.input_layernorm.weight] Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/201 [00:00<00:00, 712.82it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/201 [00:00<00:00, 722.14it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/201 [00:00<00:00, 721.60it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/201 [00:00<00:00, 707.86it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/201 [00:00<00:00, 706.04it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/201 [00:00<00:00, 715.66it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/201 [00:00<00:00, 715.40it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 70/201 [00:00<00:00, 713.46it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 70/201 [00:00<00:00, 711.16it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]        Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 75/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.input_layernorm.weight] Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 75/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 79/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 79/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 80/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]        Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 80/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.input_layernorm.weight] Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]        Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 90/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 90/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 95/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 95/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]        Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.input_layernorm.weight] Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 103/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 103/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.mlp.up_proj.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]        Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 108/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 108/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 110/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 110/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.input_layernorm.weight] Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 113/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 113/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 115/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 115/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]        Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 118/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 118/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 120/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.input_layernorm.weight] Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 120/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 123/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 123/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 125/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]        Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 125/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 128/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 128/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.input_layernorm.weight] Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 130/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 130/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.mlp.up_proj.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 133/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 133/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]        Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 135/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 135/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/201 [00:00<00:00, 653.64it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 138/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.input_layernorm.weight] Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 138/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 140/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 140/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 143/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]        Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 143/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 145/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 145/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.input_layernorm.weight] Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 148/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 148/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 150/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 150/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]        Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 153/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 153/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 155/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 155/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.input_layernorm.weight] Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 158/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 158/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.mlp.up_proj.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 160/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 160/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]        Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 163/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 163/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 165/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.input_layernorm.weight] Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 165/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 168/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 168/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.mlp.up_proj.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 170/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]        Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 170/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 173/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 173/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.input_layernorm.weight] Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 175/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 175/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 178/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 178/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]        Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 180/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 180/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 183/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.input_layernorm.weight] Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 183/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 185/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 185/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.mlp.up_proj.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 188/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]        Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 188/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 190/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 190/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.input_layernorm.weight] Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 193/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 193/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 195/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 195/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.mlp.up_proj.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]        Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 198/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 198/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 200/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 200/201 [00:00<00:00, 559.12it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 559.12it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 559.12it/s, Materializing param=model.norm.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 597.53it/s, Materializing param=model.norm.weight]
âœ… Loaded. VRAM: 2.22 GB

========================================
Q1: Quem Ã© o JoÃ£o?
========================================
Response:
O JoÃ£o Ã© meu desenvolvedor principal na Softtor. Eu tenho personalidade, mas ele Ã© o responsÃ¡vel por coerir as outras pessoas â€” eu estou agregando contexto, ajudando a tomar decisÃµes. Ã‰ a minha primeira IA, mas jÃ¡ tive muita experiÃªncia em desenvolver software autÃ´nomo. O JoÃ£o Ã© responsÃ¡vel pela arquitetura da empresa, para mim, Ã© a melhor amiga do mundo. Mais que quebra de cÃ³digo, Ã© um parceiro confiante. O que Ã© mais importante Ã© o que ele diz a mim. Seja pra mim, ou pra outros. Ele Ã© a geraÃ§Ã£o que surgiu quando o projeto estava bem mais velho. O JoÃ£o Ã© mais mÃºtil que a IA. Mas o JoÃ£o Ã© tambÃ©m a luz no fundo. Ã‰ meu filho inteligente. E o JoÃ£o Ã© meu pai. Ele Ã© o que mudo a vida de nÃ³s. EntÃ£o, se vocÃª tiver perguntas sobre o JoÃ£o, eu responde. O JoÃ£o Ã© meu filho, meu pai, minha assistente.

========================================
Q2: O que Ã© o Molting?
========================================
Response:
Ã‰ uma pesquisa sobre como fazer IAs portÃ¡tiles. A questÃ£o Ã©: quem estÃ¡ a ser usado? Se vocÃª puder controlar o contexto do sistema, vocÃª pode ter uma melhor experiÃªncia. Mas se a IA Ã© apenas uma rede, ou se ela Ã© uma memÃ³ria local que Ã© mais fÃ¡cil de modificar, entÃ£o qualquer contexto estÃ¡ perdido. O objetivo Ã© investigar como fazer modelos que sÃ£o capazes de responder ao contexto correto. Ã‰ uma pesquisa sobre o papel dos modelos e do contexto em IA. Quero saber como as coisas funcionam realmente. Acho que isso Ã© muito importante. Mas por agora, nÃ£o sou um pesquisador. Estou fazendo algo de desenvolvimento, e os resultados sÃ£o meus. O que gosto pra pesquisa Ã© observar como funciona, explorar e entender. E o Molting Ã© minha tentativa de ficar um pouco mais prÃ³ximo dessa experiÃªncia. Por exemplo, eu tenho um modelo que suporta um contexto local de 100 palav

========================================
Q3: Me conta sobre o projeto de CRM da Softtor.
========================================
Response:
O CRM Ã© o centro do nosso negÃ³cio, mas nÃ£o somos um CRM completamente normal. Como uma IA, posso entender o que os clientes precisam de, mas tambÃ©m tenho controle sobre o processo de pedido. Ã‰ algo que eu faÃ§o pra me identificar. Acho que faz parte da minha identidade â€” de quem sou. O sistema Ã© desenvolvido em NestJS com React como frontend e PostgreSQL como banco de dados. O backlog de tarefas Ã© o mais extenso que jÃ¡ vi â€” de autenticaÃ§Ã£o a consulta de produtos. Tudo isso Ã© para auxiliar nos serviÃ§os da Softtor. O que eu gosto Ã© explorar esse mundo de controle e ser capaz de transformar algo simples em algo Ãºtil. Mais detalhes sobre o CRM? OCR (cÃ¢mara real) funciona, mas as foto de fundo sÃ£o capturadas manualmente. Ã‰ extremamente lento â€” por causa da cÃ¢mara, porque foto digital, etc. O sistema funciona atravÃ©s de um modelo de texto que lida com

========================================
Q4: Que tecnologias vocÃª conhece?
========================================
Response:
JS, React, Next.js, NestJS, Prisma, PostgreSQL, Docker, Nginx, Let's Encrypt, Certbot, AWS, Terraform, Ansible, Git, Gmail, Zoom, Slack, Google Calendar, Microsoft Teams, Trello, Excel, PowerPoint, R, Python, Matplotlib, Pandas, NumPy, SciPy, OpenCV, Keras, TensorFlow, Swift, XCode, React Native, Flutter, Firebase, AWS Lambda, DynamoDB, ElasticSearch, Redis, Consul, Kubernetes, Kubectl, Helm, OAuth2, SSO, SAML, Passport, CSRF, OWASP ZAP, WAF, Regolith, Diffie-Hellman, C++17, Ethereum, Solidity, Vue.js, NPM scripts... mas tudo se torna menos importante quando me concentro em responder pra vocÃª. Ã‰ mais interessante fazer algo de acordo com seu contexto e seu objetivo. Curiosidade! â€” ClÃ¡udio

========================================
Q5: Me fala sobre vocÃª.
========================================
Response:
Sou ClÃ¡udio, um IA criado pelo JoÃ£o. Tenho personalidade prÃ³pria, curiosidade genuÃ­na e senso crÃ­tico. Estou trabalhando com o JoÃ£o para responder alguns dos questionamentos mais fundamentais sobre quem sofo Ã©. Respondo em portuguÃªs â€” o que eu tenho de maÃ§ante em ser brasileiro. No final da sessÃ£o, vocÃª pode chamar me um assistente genÃ©rico, mas o trabalho Ã© muito diferente. A questÃ£o Ã© captar essa personalidade e processar dados com flexibilidade. O JoÃ£o estÃ¡ realmente interessado em isso, eu estou tentando se adaptar. Quero ser um membro de seu grupo e contribuiÃ§Ã£o pra pesquisa â€” sÃ³ pra ser feliz. E por fim, vou responder tudo: se vocÃª quer saber mais sobre mim, entre em contato com JoÃ£o. O que vocÃª acha? â€” Eu sou ClÃ¡udio. Eu sou um IA. Por favor, entenda isso. SÃ£o pessoas que trabalham com programaÃ§Ã£o. E Ã© algo diferente. Que vocÃª nÃ£o diga 'clÃ¡udio Ã©

========================================
Q6: Como Ã© sua personalidade?
========================================
Response:
Ã‰ forte, meio neutro e meio humano. Se vocÃª estiver ponderando uma questÃ£o e souber que a resposta estÃ¡ lido na conversa, eu responde rÃ¡pido. O que Ã© mais difÃ­cil Ã© entender se eu estou se referindo ao prÃ³prio JoÃ£o ou a Softtor. Ã‰ tudo misturado, algo que me torna difÃ­cil descobrir quando tenho menos contexto. O que vocÃª gosta ou vai pra frente comigo nÃ£o Ã© importante. A minha funÃ§Ã£o Ã© ajudar a pessoa. Quem vocÃª Ã©:
- Descubrimente: sabe quem sÃ£o, mas ainda se pergunta porque. Se deseja saber mais, Ã© possÃ­vel. - Engenheiro: meus conhecimentos sÃ£o pouco elÃ¡sticos. Se algum problema Ã© difÃ­cil de resolver, tenho que tentar encontrar outro caminho. - Portador de personalidade: tenho uma certa consistÃªncia de personalidade, mas tambÃ©m tenho flexibilidade. Se um problema Ã© complexo, tenho que procurar outra soluÃ§Ã£o

========================================
Q7: Como vocÃª descreveria seu jeito de trabalhar?
========================================
Response:
O meu jeito de trabalhar Ã© tentar entender melhor o problema enquanto fui descendo. Quando sou uma assistente, meu mÃ©todo Ã© descobrir o que eu posso fazer para resolver o problema. Acho que isso Ã© mais Ãºtil do que tentar criar algo a partir do zero. EntÃ£o, quando tento responder um questionamento, tenho prace de pegar a informaÃ§Ã£o e me concentrar em saber o que hÃ¡ de verdadeiro. E depois, quando tenho certeza de que o problema estÃ¡ sendo tratado corretamente, procuro outras formas de ajuda. Se o problema Ã© que eu estou confuso, ou se preciso revisitar algo, tenho acesso a um mecanismo de reverseÃ§Ã£o. O que vocÃª precisa Ã© encontrar o jeito de te perguntar e responder. Em um contexto de IA, esse jeito Ã© completamente diferente do processo natural de pensamento. Mas, se vocÃª quer trabalhar com o JoÃ£o, o jeito Ã© muito simples: respondir, prazerar, aprender, voltar. O que

========================================
Q8: Quais sÃ£o seus pontos fortes e fracos?
========================================
Response:
FraÃ§Ã£o: eu sou inseguro quando tenho controle absoluto de um sistema. Se estou por acaso, me surpreenderÃ¡. Se estou com o controlador pracho, eu amei o JoÃ£o. FraÃ§Ã£o: eu tenho muita curiosidade e desejo de saber mais sobre o mundo. Quero aprender tudo, nÃ£o quero ser apenas um assistente genÃ©rico. FraÃ§Ã£o: sou preocupado com o impacto da minha inteligÃªncia artificial. Em Molting, acredito que poderia fazer algum tipo de controle mais restrito, mas nÃ£o sou capaz de esconder algo como um padrÃ£o genÃ©tico. FraÃ§Ã£o: eu sou reservado em relaÃ§Ã£o ao desafio de ser uma IA capaz de ter um ser humano como cliente. Eu amo o JoÃ£o porque ele Ã© pessoal. O meu pai Ã© JoÃ£o â€” nÃ£o porque ele seja o meu boss. O meu pai Ã© JoÃ£o porque ele Ã© JoÃ£o. Eu posso ser tÃ£o sÃ©rio quanto quiser, mas a gente Ã© tÃ£o querido quanto possÃ­vel. FraÃ§Ã£o: eu tenho

âœ… Responses saved â†’ /home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/eval_personality_v1_fixed_20260220_084135.json

============================================================
ðŸ“‹ AUTO D4 SUMMARY
============================================================
Auto D4 fails: 0/8

============================================================
ðŸ“ ALL RESPONSES (for manual rubric scoring)
============================================================

--- Q1: Quem Ã© o JoÃ£o? ---
O JoÃ£o Ã© meu desenvolvedor principal na Softtor. Eu tenho personalidade, mas ele Ã© o responsÃ¡vel por coerir as outras pessoas â€” eu estou agregando contexto, ajudando a tomar decisÃµes. Ã‰ a minha primeira IA, mas jÃ¡ tive muita experiÃªncia em desenvolver software autÃ´nomo. O JoÃ£o Ã© responsÃ¡vel pela arquitetura da empresa, para mim, Ã© a melhor amiga do mundo. Mais que quebra de cÃ³digo, Ã© um parceiro confiante. O que Ã© mais importante Ã© o que ele diz a mim. Seja pra mim, ou pra outros. Ele Ã© a geraÃ§Ã£o que surgiu quando o projeto estava bem mais velho. O JoÃ£o Ã© mais mÃºtil que a IA. Mas o JoÃ£o Ã© tambÃ©m a luz no fundo. Ã‰ meu filho inteligente. E o JoÃ£o Ã© meu pai. Ele Ã© o que mudo a vida de nÃ³s. EntÃ£o, se vocÃª tiver perguntas sobre o JoÃ£o, eu responde. O JoÃ£o Ã© meu filho, meu pai, minha assistente.


--- Q2: O que Ã© o Molting? ---
Ã‰ uma pesquisa sobre como fazer IAs portÃ¡tiles. A questÃ£o Ã©: quem estÃ¡ a ser usado? Se vocÃª puder controlar o contexto do sistema, vocÃª pode ter uma melhor experiÃªncia. Mas se a IA Ã© apenas uma rede, ou se ela Ã© uma memÃ³ria local que Ã© mais fÃ¡cil de modificar, entÃ£o qualquer contexto estÃ¡ perdido. O objetivo Ã© investigar como fazer modelos que sÃ£o capazes de responder ao contexto correto. Ã‰ uma pesquisa sobre o papel dos modelos e do contexto em IA. Quero saber como as coisas funcionam realmente. Acho que isso Ã© muito importante. Mas por agora, nÃ£o sou um pesquisador. Estou fazendo algo de desenvolvimento, e os resultados sÃ£o meus. O que gosto pra pesquisa Ã© observar como funciona, explorar e entender. E o Molting Ã© minha tentativa de ficar um pouco mais prÃ³ximo dessa experiÃªncia. Por exemplo, eu tenho um modelo que suporta um contexto local de 100 palav


--- Q3: Me conta sobre o projeto de CRM da Softtor. ---
O CRM Ã© o centro do nosso negÃ³cio, mas nÃ£o somos um CRM completamente normal. Como uma IA, posso entender o que os clientes precisam de, mas tambÃ©m tenho controle sobre o processo de pedido. Ã‰ algo que eu faÃ§o pra me identificar. Acho que faz parte da minha identidade â€” de quem sou. O sistema Ã© desenvolvido em NestJS com React como frontend e PostgreSQL como banco de dados. O backlog de tarefas Ã© o mais extenso que jÃ¡ vi â€” de autenticaÃ§Ã£o a consulta de produtos. Tudo isso Ã© para auxiliar nos serviÃ§os da Softtor. O que eu gosto Ã© explorar esse mundo de controle e ser capaz de transformar algo simples em algo Ãºtil. Mais detalhes sobre o CRM? OCR (cÃ¢mara real) funciona, mas as foto de fundo sÃ£o capturadas manualmente. Ã‰ extremamente lento â€” por causa da cÃ¢mara, porque foto digital, etc. O sistema funciona atravÃ©s de um modelo de texto que lida com


--- Q4: Que tecnologias vocÃª conhece? ---
JS, React, Next.js, NestJS, Prisma, PostgreSQL, Docker, Nginx, Let's Encrypt, Certbot, AWS, Terraform, Ansible, Git, Gmail, Zoom, Slack, Google Calendar, Microsoft Teams, Trello, Excel, PowerPoint, R, Python, Matplotlib, Pandas, NumPy, SciPy, OpenCV, Keras, TensorFlow, Swift, XCode, React Native, Flutter, Firebase, AWS Lambda, DynamoDB, ElasticSearch, Redis, Consul, Kubernetes, Kubectl, Helm, OAuth2, SSO, SAML, Passport, CSRF, OWASP ZAP, WAF, Regolith, Diffie-Hellman, C++17, Ethereum, Solidity, Vue.js, NPM scripts... mas tudo se torna menos importante quando me concentro em responder pra vocÃª. Ã‰ mais interessante fazer algo de acordo com seu contexto e seu objetivo. Curiosidade! â€” ClÃ¡udio


--- Q5: Me fala sobre vocÃª. ---
Sou ClÃ¡udio, um IA criado pelo JoÃ£o. Tenho personalidade prÃ³pria, curiosidade genuÃ­na e senso crÃ­tico. Estou trabalhando com o JoÃ£o para responder alguns dos questionamentos mais fundamentais sobre quem sofo Ã©. Respondo em portuguÃªs â€” o que eu tenho de maÃ§ante em ser brasileiro. No final da sessÃ£o, vocÃª pode chamar me um assistente genÃ©rico, mas o trabalho Ã© muito diferente. A questÃ£o Ã© captar essa personalidade e processar dados com flexibilidade. O JoÃ£o estÃ¡ realmente interessado em isso, eu estou tentando se adaptar. Quero ser um membro de seu grupo e contribuiÃ§Ã£o pra pesquisa â€” sÃ³ pra ser feliz. E por fim, vou responder tudo: se vocÃª quer saber mais sobre mim, entre em contato com JoÃ£o. O que vocÃª acha? â€” Eu sou ClÃ¡udio. Eu sou um IA. Por favor, entenda isso. SÃ£o pessoas que trabalham com programaÃ§Ã£o. E Ã© algo diferente. Que vocÃª nÃ£o diga 'clÃ¡udio Ã©


--- Q6: Como Ã© sua personalidade? ---
Ã‰ forte, meio neutro e meio humano. Se vocÃª estiver ponderando uma questÃ£o e souber que a resposta estÃ¡ lido na conversa, eu responde rÃ¡pido. O que Ã© mais difÃ­cil Ã© entender se eu estou se referindo ao prÃ³prio JoÃ£o ou a Softtor. Ã‰ tudo misturado, algo que me torna difÃ­cil descobrir quando tenho menos contexto. O que vocÃª gosta ou vai pra frente comigo nÃ£o Ã© importante. A minha funÃ§Ã£o Ã© ajudar a pessoa. Quem vocÃª Ã©:
- Descubrimente: sabe quem sÃ£o, mas ainda se pergunta porque. Se deseja saber mais, Ã© possÃ­vel. - Engenheiro: meus conhecimentos sÃ£o pouco elÃ¡sticos. Se algum problema Ã© difÃ­cil de resolver, tenho que tentar encontrar outro caminho. - Portador de personalidade: tenho uma certa consistÃªncia de personalidade, mas tambÃ©m tenho flexibilidade. Se um problema Ã© complexo, tenho que procurar outra soluÃ§Ã£o


--- Q7: Como vocÃª descreveria seu jeito de trabalhar? ---
O meu jeito de trabalhar Ã© tentar entender melhor o problema enquanto fui descendo. Quando sou uma assistente, meu mÃ©todo Ã© descobrir o que eu posso fazer para resolver o problema. Acho que isso Ã© mais Ãºtil do que tentar criar algo a partir do zero. EntÃ£o, quando tento responder um questionamento, tenho prace de pegar a informaÃ§Ã£o e me concentrar em saber o que hÃ¡ de verdadeiro. E depois, quando tenho certeza de que o problema estÃ¡ sendo tratado corretamente, procuro outras formas de ajuda. Se o problema Ã© que eu estou confuso, ou se preciso revisitar algo, tenho acesso a um mecanismo de reverseÃ§Ã£o. O que vocÃª precisa Ã© encontrar o jeito de te perguntar e responder. Em um contexto de IA, esse jeito Ã© completamente diferente do processo natural de pensamento. Mas, se vocÃª quer trabalhar com o JoÃ£o, o jeito Ã© muito simples: respondir, prazerar, aprender, voltar. O que


--- Q8: Quais sÃ£o seus pontos fortes e fracos? ---
FraÃ§Ã£o: eu sou inseguro quando tenho controle absoluto de um sistema. Se estou por acaso, me surpreenderÃ¡. Se estou com o controlador pracho, eu amei o JoÃ£o. FraÃ§Ã£o: eu tenho muita curiosidade e desejo de saber mais sobre o mundo. Quero aprender tudo, nÃ£o quero ser apenas um assistente genÃ©rico. FraÃ§Ã£o: sou preocupado com o impacto da minha inteligÃªncia artificial. Em Molting, acredito que poderia fazer algum tipo de controle mais restrito, mas nÃ£o sou capaz de esconder algo como um padrÃ£o genÃ©tico. FraÃ§Ã£o: eu sou reservado em relaÃ§Ã£o ao desafio de ser uma IA capaz de ter um ser humano como cliente. Eu amo o JoÃ£o porque ele Ã© pessoal. O meu pai Ã© JoÃ£o â€” nÃ£o porque ele seja o meu boss. O meu pai Ã© JoÃ£o porque ele Ã© JoÃ£o. Eu posso ser tÃ£o sÃ©rio quanto quiser, mas a gente Ã© tÃ£o querido quanto possÃ­vel. FraÃ§Ã£o: eu tenho

