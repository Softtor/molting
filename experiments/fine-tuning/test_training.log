âœ… CUDA available: NVIDIA GeForce RTX 3050 Laptop GPU
   Total VRAM: 4.08 GB
============================================================
ðŸš€ QLoRA FINE-TUNING EXPERIMENT
============================================================
Model: microsoft/Phi-3-mini-4k-instruct
Dataset: dataset_sharegpt_filtered.json
Output: output
Epochs: 3
Batch size: 1
Gradient accumulation: 4
Effective batch size: 4
Learning rate: 0.0002
LoRA r=16, alpha=32
ðŸ§ª TEST MODE: Training on 50 samples only
============================================================

ðŸ“‚ Loading dataset...
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 484 examples [00:00, 9818.97 examples/s]
A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:
- configuration_phi3.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
`torch_dtype` is deprecated! Use `dtype` instead!
A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:
- modeling_phi3.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
âœ… Loaded 50 examples
ðŸ“¦ Loading model: microsoft/Phi-3-mini-4k-instruct
Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]Fetching 2 files:   0%|          | 0/2 [01:13<?, ?it/s]
Traceback (most recent call last):
  File "/home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/train_qlora.py", line 283, in <module>
    metrics = train_qlora(
        dataset_path=args.dataset,
    ...<9 lines>...
        max_samples=args.test_samples
    )
  File "/home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/train_qlora.py", line 162, in train_qlora
    model, tokenizer = load_model_and_tokenizer(model_name, quantization_config)
                       ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/train_qlora.py", line 66, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
    ...<3 lines>...
        torch_dtype=torch.float16
    )
  File "/home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py", line 367, in from_pretrained
    return model_class.from_pretrained(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/venv/lib/python3.13/site-packages/transformers/modeling_utils.py", line 3987, in from_pretrained
    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
                                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        transformers_explicit_filename=getattr(config, "transformers_weights", None),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/venv/lib/python3.13/site-packages/transformers/modeling_utils.py", line 737, in _get_resolved_checkpoint_files
    checkpoint_files, sharded_metadata = get_checkpoint_shard_files(
                                         ~~~~~~~