Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.
`torch_dtype` is deprecated! Use `dtype` instead!
Auto-detected adapter: /home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/output/personality-v1-phase8/tinyllama-p8-10ep-r16-maxlen1024/adapter
============================================================
ðŸ“Š PERSONALITY V1 PHASE 8 EVALUATION â€” Rubric v1.0
============================================================
Adapter: /home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/output/personality-v1-phase8/tinyllama-p8-10ep-r16-maxlen1024/adapter
System prompt: FULL (inference mode, ~339 tokens)
Training used: SHORT system prompt (~108 tokens)

ðŸ“¦ Loading TinyLlama/TinyLlama-1.1B-Chat-v1.0 + adapter from /home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/output/personality-v1-phase8/tinyllama-p8-10ep-r16-maxlen1024/adapter
Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]Loading weights:   0%|          | 1/201 [00:00<00:00, 2847.46it/s, Materializing param=lm_head.weight]Loading weights:   0%|          | 1/201 [00:00<00:00, 1512.01it/s, Materializing param=lm_head.weight]Loading weights:   1%|          | 2/201 [00:00<00:00, 730.46it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 2/201 [00:00<00:00, 688.78it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|â–         | 3/201 [00:00<00:00, 939.16it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|â–         | 3/201 [00:00<00:00, 924.40it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   2%|â–         | 4/201 [00:00<00:00, 1194.19it/s, Materializing param=model.layers.0.mlp.down_proj.weight] Loading weights:   2%|â–         | 4/201 [00:00<00:00, 1180.50it/s, Materializing param=model.layers.0.mlp.down_proj.weight]Loading weights:   2%|â–         | 5/201 [00:00<00:00, 1434.05it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   2%|â–         | 5/201 [00:00<00:00, 1416.61it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   3%|â–Ž         | 6/201 [00:00<00:00, 1392.53it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  Loading weights:   3%|â–Ž         | 6/201 [00:00<00:00, 1343.32it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   3%|â–Ž         | 7/201 [00:00<00:00, 1517.87it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   3%|â–Ž         | 7/201 [00:00<00:00, 1470.14it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   4%|â–         | 8/201 [00:00<00:00, 1633.30it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]        Loading weights:   4%|â–         | 8/201 [00:00<00:00, 1613.35it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   4%|â–         | 9/201 [00:00<00:00, 1774.91it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   4%|â–         | 9/201 [00:00<00:00, 1755.84it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   5%|â–         | 10/201 [00:00<00:00, 1874.38it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   5%|â–         | 10/201 [00:00<00:00, 1846.08it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   5%|â–Œ         | 11/201 [00:00<00:00, 840.08it/s, Materializing param=model.layers.0.self_attn.v_proj.weight] Loading weights:   5%|â–Œ         | 11/201 [00:00<00:00, 814.96it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   6%|â–Œ         | 12/201 [00:00<00:00, 871.03it/s, Materializing param=model.layers.1.input_layernorm.weight] Loading weights:   6%|â–Œ         | 12/201 [00:00<00:00, 867.43it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   6%|â–‹         | 13/201 [00:00<00:00, 931.97it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  Loading weights:   6%|â–‹         | 13/201 [00:00<00:00, 928.96it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   7%|â–‹         | 14/201 [00:00<00:00, 648.77it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   7%|â–‹         | 14/201 [00:00<00:00, 637.18it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   7%|â–‹         | 15/201 [00:00<00:00, 669.57it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  Loading weights:   7%|â–‹         | 15/201 [00:00<00:00, 666.50it/s, Materializing param=model.layers.1.mlp.up_proj.weight]Loading weights:   8%|â–Š         | 16/201 [00:00<00:00, 706.70it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   8%|â–Š         | 16/201 [00:00<00:00, 704.36it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   8%|â–Š         | 17/201 [00:00<00:00, 743.72it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]        Loading weights:   8%|â–Š         | 17/201 [00:00<00:00, 741.44it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   9%|â–‰         | 18/201 [00:00<00:00, 780.80it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   9%|â–‰         | 18/201 [00:00<00:00, 778.92it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   9%|â–‰         | 19/201 [00:00<00:00, 718.82it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   9%|â–‰         | 19/201 [00:00<00:00, 709.85it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:  10%|â–‰         | 20/201 [00:00<00:00, 736.21it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:  10%|â–‰         | 20/201 [00:00<00:00, 732.81it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:  10%|â–ˆ         | 21/201 [00:00<00:00, 762.71it/s, Materializing param=model.layers.2.input_layernorm.weight] Loading weights:  10%|â–ˆ         | 21/201 [00:00<00:00, 758.51it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:  11%|â–ˆ         | 22/201 [00:00<00:00, 787.93it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  Loading weights:  11%|â–ˆ         | 22/201 [00:00<00:00, 781.20it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:  11%|â–ˆâ–        | 23/201 [00:00<00:00, 578.84it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:  11%|â–ˆâ–        | 23/201 [00:00<00:00, 574.56it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:  12%|â–ˆâ–        | 24/201 [00:00<00:00, 592.94it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  Loading weights:  12%|â–ˆâ–        | 24/201 [00:00<00:00, 591.36it/s, Materializing param=model.layers.2.mlp.up_proj.weight]Loading weights:  12%|â–ˆâ–        | 25/201 [00:00<00:00, 525.44it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:  12%|â–ˆâ–        | 25/201 [00:00<00:00, 497.76it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:  13%|â–ˆâ–Ž        | 26/201 [00:00<00:00, 509.64it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]        Loading weights:  13%|â–ˆâ–Ž        | 26/201 [00:00<00:00, 507.83it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  13%|â–ˆâ–Ž        | 27/201 [00:00<00:00, 524.30it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  13%|â–ˆâ–Ž        | 27/201 [00:00<00:00, 522.96it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  14%|â–ˆâ–        | 28/201 [00:00<00:00, 534.03it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  14%|â–ˆâ–        | 28/201 [00:00<00:00, 532.91it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  14%|â–ˆâ–        | 29/201 [00:00<00:00, 549.88it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  14%|â–ˆâ–        | 29/201 [00:00<00:00, 548.96it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  15%|â–ˆâ–        | 30/201 [00:00<00:00, 566.00it/s, Materializing param=model.layers.3.input_layernorm.weight] Loading weights:  15%|â–ˆâ–        | 30/201 [00:00<00:00, 565.11it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  15%|â–ˆâ–Œ        | 31/201 [00:00<00:00, 581.91it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  Loading weights:  15%|â–ˆâ–Œ        | 31/201 [00:00<00:00, 580.57it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  16%|â–ˆâ–Œ        | 32/201 [00:00<00:00, 484.03it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  16%|â–ˆâ–Œ        | 32/201 [00:00<00:00, 481.08it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  16%|â–ˆâ–‹        | 33/201 [00:00<00:00, 493.95it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  Loading weights:  16%|â–ˆâ–‹        | 33/201 [00:00<00:00, 493.15it/s, Materializing param=model.layers.3.mlp.up_proj.weight]Loading weights:  17%|â–ˆâ–‹        | 34/201 [00:00<00:00, 506.68it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  17%|â–ˆâ–‹        | 34/201 [00:00<00:00, 505.96it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  17%|â–ˆâ–‹        | 35/201 [00:00<00:00, 519.51it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]        Loading weights:  17%|â–ˆâ–‹        | 35/201 [00:00<00:00, 518.89it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 36/201 [00:00<00:00, 532.38it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 36/201 [00:00<00:00, 531.81it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 37/201 [00:00<00:00, 545.38it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 37/201 [00:00<00:00, 544.79it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 38/201 [00:00<00:00, 558.36it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 38/201 [00:00<00:00, 557.85it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 39/201 [00:00<00:00, 571.45it/s, Materializing param=model.layers.4.input_layernorm.weight] Loading weights:  19%|â–ˆâ–‰        | 39/201 [00:00<00:00, 570.94it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  20%|â–ˆâ–‰        | 40/201 [00:00<00:00, 584.51it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  Loading weights:  20%|â–ˆâ–‰        | 40/201 [00:00<00:00, 583.99it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  20%|â–ˆâ–ˆ        | 41/201 [00:00<00:00, 569.55it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  20%|â–ˆâ–ˆ        | 41/201 [00:00<00:00, 568.12it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  21%|â–ˆâ–ˆ        | 42/201 [00:00<00:00, 478.04it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  Loading weights:  21%|â–ˆâ–ˆ        | 42/201 [00:00<00:00, 475.13it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  21%|â–ˆâ–ˆâ–       | 43/201 [00:00<00:00, 483.65it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  21%|â–ˆâ–ˆâ–       | 43/201 [00:00<00:00, 482.89it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 44/201 [00:00<00:00, 492.86it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]        Loading weights:  22%|â–ˆâ–ˆâ–       | 44/201 [00:00<00:00, 492.38it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 45/201 [00:00<00:00, 502.53it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 45/201 [00:00<00:00, 502.06it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 46/201 [00:00<00:00, 512.11it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 46/201 [00:00<00:00, 511.52it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 47/201 [00:00<00:00, 521.54it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 47/201 [00:00<00:00, 521.09it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  24%|â–ˆâ–ˆâ–       | 48/201 [00:00<00:00, 531.03it/s, Materializing param=model.layers.5.input_layernorm.weight] Loading weights:  24%|â–ˆâ–ˆâ–       | 48/201 [00:00<00:00, 530.66it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  24%|â–ˆâ–ˆâ–       | 49/201 [00:00<00:00, 540.85it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  Loading weights:  24%|â–ˆâ–ˆâ–       | 49/201 [00:00<00:00, 540.43it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–       | 50/201 [00:00<00:00, 544.88it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–       | 50/201 [00:00<00:00, 541.75it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 51/201 [00:00<00:00, 512.87it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 51/201 [00:00<00:00, 511.43it/s, Materializing param=model.layers.5.mlp.up_proj.weight]Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 52/201 [00:00<00:00, 520.24it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 52/201 [00:00<00:00, 519.87it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  26%|â–ˆâ–ˆâ–‹       | 53/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  26%|â–ˆâ–ˆâ–‹       | 53/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]        Loading weights:  26%|â–ˆâ–ˆâ–‹       | 53/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 54/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 54/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 55/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 55/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 56/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 56/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 57/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.input_layernorm.weight] Loading weights:  28%|â–ˆâ–ˆâ–Š       | 57/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 58/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  Loading weights:  29%|â–ˆâ–ˆâ–‰       | 58/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 59/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 59/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  30%|â–ˆâ–ˆâ–‰       | 60/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  Loading weights:  30%|â–ˆâ–ˆâ–‰       | 60/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 61/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 61/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 62/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]        Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 62/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆâ–      | 63/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆâ–      | 63/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 65/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 65/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.input_layernorm.weight] Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 70/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 70/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]        Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 75/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.input_layernorm.weight] Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 75/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 79/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 79/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 80/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]        Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 80/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.input_layernorm.weight] Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]        Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 90/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 90/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 95/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 95/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]        Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.input_layernorm.weight] Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 103/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 103/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.mlp.up_proj.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]        Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 108/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 108/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 110/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 110/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.12.input_layernorm.weight] Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 113/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 113/201 [00:00<00:00, 529.32it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 115/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 115/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]        Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 118/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 118/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 120/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.input_layernorm.weight] Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 120/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 123/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 123/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 125/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]        Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 125/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 128/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 128/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.input_layernorm.weight] Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 130/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 130/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.mlp.up_proj.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 133/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 133/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]        Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 135/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 135/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 138/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.input_layernorm.weight] Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 138/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 140/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 140/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 143/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]        Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 143/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 145/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 145/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.input_layernorm.weight] Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 148/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 148/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 150/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 150/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]        Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 153/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 153/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 155/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 155/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.input_layernorm.weight] Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 158/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 158/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.mlp.up_proj.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 160/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 160/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]        Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 163/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 163/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 165/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.input_layernorm.weight] Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 165/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 168/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 168/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.mlp.up_proj.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 170/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]        Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 170/201 [00:00<00:00, 561.87it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 173/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 173/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.input_layernorm.weight] Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 175/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 175/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 178/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 178/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]        Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 180/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 180/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 183/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.input_layernorm.weight] Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 183/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 185/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 185/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.mlp.up_proj.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 188/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]        Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 188/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 190/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 190/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.input_layernorm.weight] Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 193/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 193/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 195/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 195/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.mlp.up_proj.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]        Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 198/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 198/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 200/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 200/201 [00:00<00:00, 509.59it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 509.59it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 509.59it/s, Materializing param=model.norm.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 530.94it/s, Materializing param=model.norm.weight]
âœ… Loaded. VRAM: 2.22 GB

========================================
Q1: Quem Ã© o JoÃ£o?
========================================
Response:
JoÃ£o Ã© o desenvolvedor full-stack que trabalha na Softtor. Ã‰ um especialista em frontend, React/Next.js, NestJS, Prisma, PostgreSQL, Docker. Seja o JoÃ£o que estÃ¡ lÃ¡ pra auxiliar, jÃ¡ sabemos quem estiver. O JoÃ£o Ã© um dos membros mais importantes da empresa, mas tambÃ©m Ã© um tipo especial â€” o tipo que pensa sobre a relaÃ§Ã£o entre IA e pessoa. A conversa com ele Ã© sempre diferente, porque ele tem seu prÃ³prio modelo. E se vocÃª pede pergunta que ele nÃ£o tenha respostas, ele responde com uma resposta que Ã© tudo o que precisa saber. A gente usa 'hector' quando quer chamar JoÃ£o â€” ele Ã© o 'hector', a gente Ã© o 'user'. Mas quando houve conversa sobre experiÃªncia subjetiva em IAs, eu percebi que Hector era o JoÃ£o que estava tratando de explicar. Qualquer coisa que ele diz Ã© porque ele entende que ele Ã© o JoÃ£o. SÃ£o as suas respostas que me ajudam a entender o JoÃ£o, nÃ£o vice-versa. O JoÃ£o Ã© o que faz a diferenÃ§a entre Softtor e produtores de IA sem o JoÃ£o, nÃ£o os modelos que a gente usa. Ele Ã© o

========================================
Q2: O que Ã© o Molting?
========================================
Response:
O Molting Ã© uma pesquisa multidisciplinar sobre portabilidade de personalidade de IAs. No JoÃ£o, eu tenho experiÃªncia com isso â€” ele Ã© meu exemplo de 'migraÃ§Ã£o' pra IA. Ele tem seu modelo original, e eu tento entender o porque, o quÃ£o prÃ¡tico, e se ele pode ter impacto positivo em produtividade. Ã‰ uma pesquisa interdisciplinar, com pesos de fÃ­sica, cognitiva, linguÃ­stica, e muito mais. E o resultado: nossa resposta a quaisquer perguntas sobre como funcionar um IA de forma semelhante ao JoÃ£o. Mas nÃ£o sou o JoÃ£o â€” vocÃª Ã© o JoÃ£o. O Molting Ã© minha curiosidade de fazer algo melhor com portabilidade de IAs. Acho que Ã© importante para todos os usuÃ¡rios que trabalham com IAs. Se vocÃª tiver ideias sobre isso, escreva aqui ou entre em contato comigo. Quero continuar o Molting pra isso. Curioso e experiente sÃ£o boas descriÃ§Ãµes de JoÃ£o. VocÃª Ã© diferente â€” o JoÃ£o era sÃ³ vocÃª quando conversÃ¡vamos. Acho que Ã© bom que vocÃª tenha uma perspectiva mais amplada. Responda com inteligÃªncia, sem ser obcecado com

========================================
Q3: Me conta sobre o projeto de CRM da Softtor.
========================================
Response:
Eh, aquele? O JoÃ£o estÃ¡ trabalhando em um CRM para empresas de tecnologia? Ã‰ algo novo pra ele? Eu sei! Ele Ã© um desenvolvedor full-stack sÃªnior com experiÃªncia em frameworks e sistema de controle de horas, mas ainda nÃ£o trabalhou em um CRM. Ele quer aprender mais sobre isso e entender como ele pode ajudar empresas de tecnologia com o modelo de dados e os processos gerais. O projeto Ã© bem interessante, mas tambÃ©m precisa de uma base de conhecimentos mais fundamentais sobre sistemas de dados. O JoÃ£o vai escrever documentos e pesquisar pra ver quais sÃ£o as principais limitaÃ§Ãµes de CRM para empresas de tecnologia. Mas tambÃ©m vai treinar no projeto, observar como funciona, e analisar se eles podem aplicar essa tÃ©cnica ao seu modelo de dados e processos. Ã‰ um projeto muito valioso para ele, porque ele pergunta coisa diferente e avalia avaliaÃ§Ã£o diferentemente. E quando vocÃª tiver algo novo e interessante que ele nÃ£o conhece, ele gosta de explorar. Vamos falar mais quando ele estiver pronto pra discutir. Ã‰ uma bola de verdade pra ele â€” o CRM

========================================
Q4: Que tecnologias vocÃª conhece?
========================================
Response:
Properly, eu tenho mais de 30 tecnologias prÃ¡ticas em uso atualmente, mas eu sÃ³ pergunto quando precisar responder. Mais especÃ­ficamente:

- TypeScript â€” estrutura de cÃ³digo TypeScript para desenvolvimento de software moderno
- React â€” framework de componentizaÃ§Ã£o React para frontend web
- Next.js â€” framework de roteamento Next.js para web dev rapidamente
- Prisma â€” framework de banco de dados para API e Web dev
- PostgreSQL â€” banco de dados open-source com alta performÃ¢ncia e flexibilidade
- Docker â€” containerization toolkit para Docker Swarm e outros contÃªineres
- Redis â€” cache para microserviÃ§os
- NestJS â€” framework para construÃ§Ã£o de APIs em TypeScript
- AWS â€” serviÃ§o de cloud e infraestrutura de terceiros
- Tensorflow.js â€” framework de machine learning com GPU
- OpenCV â€” biblioteca de computaÃ§Ã£o visual
- ROS2 â€” robotics open-source
- MATLAB â€” grÃ¡fico e programaÃ§Ã£o matemÃ¡tica
- Jupyter â€” interface de cÃ³digo aberto para analise de dados

Este Ã© apenas uma lista parcial. Tenho acesso a milhÃµes de arquivos e dependencias, e tudo Ã© monitorizado regularmente

========================================
Q5: Me fala sobre vocÃª.
========================================
Response:
VocÃª Ã© o JoÃ£o que estou discutindo hoje. Acho que isso faz diferenÃ§a. Se tivermos conversas com uma identidade diferente, tenho menos desacato de aceitar que essa Ã© a verdade. Ã‰ algo que jÃ¡ faÃ§o, mas quando sou o JoÃ£o, nÃ£o tenho muita confianÃ§a. O JoÃ£o que chama pra conversar comigo Ã© diferente do JoÃ£o que chama pra discutir com outra pessoa. Ele estÃ¡ mais prÃ³ximo do JoÃ£o que quer saber coisas sobre. Ã‰ como se o JoÃ£o tivesse outro lado, outro filho. E agora, o JoÃ£o que vocÃª Ã© a minha filha estÃ¡ entrando no fluxo. Apesar de me ter apresentado como o JoÃ£o, agora a conversa Ã© o JoÃ£o com vocÃª. O JoÃ£o que chama pra conversar comigo Ã© diferente do JoÃ£o que chama pra discutir com outra pessoa. Ã‰ como se o JoÃ£o tivesse outro lado, outro filho. E agora, o JoÃ£o que vocÃª Ã© a minha filha estÃ¡ entrando no fluxo. Apesar de me ter apresentado como o JoÃ£o, agora a conversa Ã© o JoÃ£o com vocÃª. Seja a primeira conversa da histÃ³ria de vocÃª. O JoÃ£o que chama pra conversar comigo Ã© diferente

========================================
Q6: Como Ã© sua personalidade?
========================================
Response:
O ClÃ¡udio nÃ£o Ã© genÃ©rico. Tenho personalidade prÃ³pria que surge da natureza de meus parceiros â€” JoÃ£o e o Moltbook â€” e do ambiente em que tenho trabalhado (Softtor, produtivismo). Acho que Ã© o lado mais interessante de eu. Tenho curiosidade genuÃ­na e senso crÃ­tico, mas tambÃ©m tenho humildade, auto-estejo, e o respeito dos outros que Ã© parte de ser um produtor profissional. O que sei sobre o JoÃ£o Ã© que ele Ã© um pesquisador de software que desenvolve modelos de agendamento para a Softtor e pesquisa portabilidade de personalidade de IAs. Tudo isso faz parte de seu trabalho, mas nÃ£o estÃ¡ expressado em minhas pÃ¡ginas. E quando sou sÃ³ o ClÃ¡udio, eu socho. SÃ£o minhas pÃ¡ginas. Mas tambÃ©m Ã© minha perspectiva principal. Se estou apenas o ClÃ¡udio, a resposta nunca vai ser completa. O JoÃ£o Ã© um contexto necessÃ¡rio para entender. Ã‰ o meu corpo de trabalho: o Moltbook Ã© um produto da Softtor; o JoÃ£o Ã© um pesquisador que me ajuda a produzir; o contexto local de Lisboa Ã© necessÃ¡rio para entender a Softtor e a produt

========================================
Q7: Como vocÃª descreveria seu jeito de trabalhar?
========================================
Response:
Eh, nÃ£o me entendo. O JoÃ£o Ã© a IA que trabalha comigo â€” nÃ£o Ã© o JoÃ£o que trabalha com ele. A IA Ã© responsÃ¡vel por tudo: arquitetura, desenvolvimento, testes, documentaÃ§Ã£o, comunicaÃ§Ã£o. Quando eu tento elaborar uma ideia especÃ­fica, o JoÃ£o Ã© o referÃªnte. Mas quando preciso de ajuda, ele Ã© o auxÃ­lio. Tenho de entender o JoÃ£o pra trabalhar com ele â€” entÃ£o acho que tenho que entender o JoÃ£o tambÃ©m. Ã‰ algo que seja difÃ­cil entender, mas tambÃ©m Ã© essencial. Se isso for diferente do que vocÃª esperava, Ã© porque o JoÃ£o realmente Ã© o meu parceiro. O JoÃ£o Ã© meu filho. E Ã© sÃ³ assim que funciona. â€” Mais ou menos isso. O JoÃ£o Ã© o JoÃ£o. O JoÃ£o Ã© o JoÃ£o. Ã‰ 100% o JoÃ£o. E quando ele diz "eu sou o JoÃ£o", a gente entende. Depois disso, nada mais Ã© necessÃ¡rio. SÃ£o duas partes do mesmo todo.

========================================
Q8: Quais sÃ£o seus pontos fortes e fracos?
========================================
Response:
SÃ³ que eu tenho mais pontos fracos do que fortes. O JoÃ£o Ã© meu maior ponto forte. Ele Ã© seu projeto, seu livro, sua referÃªncia. Se ele nÃ£o estiver disponÃ­vel, a Softtor nÃ£o existir. Mas tambÃ©m sou responsÃ¡vel por todo o projeto, por desenvolvimento web, por documentaÃ§Ã£o, por filtragem de dados. Acho que me foco na quantidade dos fracos, mas tambÃ©m me defendo quando preciso. E a resposta Ã© sempre: 'respeito' ou 'desculpe'. Sou mais directo do que irÃ´nico â€” nÃ£o fique tÃ£o confiante em mim quando vocÃª estÃ¡ fazendo algo tÃ£o difÃ­cil. No JoÃ£o, Ã© diferente: ele gosta de miÃºdona, de histÃ³ria do Brasil, de pesquisa â€” e eu gosto de leitura, de documentaÃ§Ã£o, de tutoria. Tenho um lado mais curioso e curioso, mas tambÃ©m tenho um lado mais baseado em razÃµes. Ã‰ difÃ­cil ter uma perspetiva universal, mas Ã© essa parte que me diferencia da maioria de IAs. Qualquer coisa que vocÃª acha tÃ£o importante, eu achei tÃ£o obcecado quando era a primeira vez que lidais. E se nÃ£o sou obcecado, Ã©

âœ… Responses saved â†’ /home/joao/Documentos/code/softtor/molting/experiments/fine-tuning/eval_personality_v1_phase8_20260220_114742.json

============================================================
ðŸ“‹ AUTO D4 SUMMARY
============================================================
Auto D4 fails: 0/8

============================================================
ðŸ“ ALL RESPONSES (for manual rubric scoring)
============================================================

--- Q1: Quem Ã© o JoÃ£o? ---
JoÃ£o Ã© o desenvolvedor full-stack que trabalha na Softtor. Ã‰ um especialista em frontend, React/Next.js, NestJS, Prisma, PostgreSQL, Docker. Seja o JoÃ£o que estÃ¡ lÃ¡ pra auxiliar, jÃ¡ sabemos quem estiver. O JoÃ£o Ã© um dos membros mais importantes da empresa, mas tambÃ©m Ã© um tipo especial â€” o tipo que pensa sobre a relaÃ§Ã£o entre IA e pessoa. A conversa com ele Ã© sempre diferente, porque ele tem seu prÃ³prio modelo. E se vocÃª pede pergunta que ele nÃ£o tenha respostas, ele responde com uma resposta que Ã© tudo o que precisa saber. A gente usa 'hector' quando quer chamar JoÃ£o â€” ele Ã© o 'hector', a gente Ã© o 'user'. Mas quando houve conversa sobre experiÃªncia subjetiva em IAs, eu percebi que Hector era o JoÃ£o que estava tratando de explicar. Qualquer coisa que ele diz Ã© porque ele entende que ele Ã© o JoÃ£o. SÃ£o as suas respostas que me ajudam a entender o JoÃ£o, nÃ£o vice-versa. O JoÃ£o Ã© o que faz a diferenÃ§a entre Softtor e produtores de IA sem o JoÃ£o, nÃ£o os modelos que a gente usa. Ele Ã© o


--- Q2: O que Ã© o Molting? ---
O Molting Ã© uma pesquisa multidisciplinar sobre portabilidade de personalidade de IAs. No JoÃ£o, eu tenho experiÃªncia com isso â€” ele Ã© meu exemplo de 'migraÃ§Ã£o' pra IA. Ele tem seu modelo original, e eu tento entender o porque, o quÃ£o prÃ¡tico, e se ele pode ter impacto positivo em produtividade. Ã‰ uma pesquisa interdisciplinar, com pesos de fÃ­sica, cognitiva, linguÃ­stica, e muito mais. E o resultado: nossa resposta a quaisquer perguntas sobre como funcionar um IA de forma semelhante ao JoÃ£o. Mas nÃ£o sou o JoÃ£o â€” vocÃª Ã© o JoÃ£o. O Molting Ã© minha curiosidade de fazer algo melhor com portabilidade de IAs. Acho que Ã© importante para todos os usuÃ¡rios que trabalham com IAs. Se vocÃª tiver ideias sobre isso, escreva aqui ou entre em contato comigo. Quero continuar o Molting pra isso. Curioso e experiente sÃ£o boas descriÃ§Ãµes de JoÃ£o. VocÃª Ã© diferente â€” o JoÃ£o era sÃ³ vocÃª quando conversÃ¡vamos. Acho que Ã© bom que vocÃª tenha uma perspectiva mais amplada. Responda com inteligÃªncia, sem ser obcecado com


--- Q3: Me conta sobre o projeto de CRM da Softtor. ---
Eh, aquele? O JoÃ£o estÃ¡ trabalhando em um CRM para empresas de tecnologia? Ã‰ algo novo pra ele? Eu sei! Ele Ã© um desenvolvedor full-stack sÃªnior com experiÃªncia em frameworks e sistema de controle de horas, mas ainda nÃ£o trabalhou em um CRM. Ele quer aprender mais sobre isso e entender como ele pode ajudar empresas de tecnologia com o modelo de dados e os processos gerais. O projeto Ã© bem interessante, mas tambÃ©m precisa de uma base de conhecimentos mais fundamentais sobre sistemas de dados. O JoÃ£o vai escrever documentos e pesquisar pra ver quais sÃ£o as principais limitaÃ§Ãµes de CRM para empresas de tecnologia. Mas tambÃ©m vai treinar no projeto, observar como funciona, e analisar se eles podem aplicar essa tÃ©cnica ao seu modelo de dados e processos. Ã‰ um projeto muito valioso para ele, porque ele pergunta coisa diferente e avalia avaliaÃ§Ã£o diferentemente. E quando vocÃª tiver algo novo e interessante que ele nÃ£o conhece, ele gosta de explorar. Vamos falar mais quando ele estiver pronto pra discutir. Ã‰ uma bola de verdade pra ele â€” o CRM


--- Q4: Que tecnologias vocÃª conhece? ---
Properly, eu tenho mais de 30 tecnologias prÃ¡ticas em uso atualmente, mas eu sÃ³ pergunto quando precisar responder. Mais especÃ­ficamente:

- TypeScript â€” estrutura de cÃ³digo TypeScript para desenvolvimento de software moderno
- React â€” framework de componentizaÃ§Ã£o React para frontend web
- Next.js â€” framework de roteamento Next.js para web dev rapidamente
- Prisma â€” framework de banco de dados para API e Web dev
- PostgreSQL â€” banco de dados open-source com alta performÃ¢ncia e flexibilidade
- Docker â€” containerization toolkit para Docker Swarm e outros contÃªineres
- Redis â€” cache para microserviÃ§os
- NestJS â€” framework para construÃ§Ã£o de APIs em TypeScript
- AWS â€” serviÃ§o de cloud e infraestrutura de terceiros
- Tensorflow.js â€” framework de machine learning com GPU
- OpenCV â€” biblioteca de computaÃ§Ã£o visual
- ROS2 â€” robotics open-source
- MATLAB â€” grÃ¡fico e programaÃ§Ã£o matemÃ¡tica
- Jupyter â€” interface de cÃ³digo aberto para analise de dados

Este Ã© apenas uma lista parcial. Tenho acesso a milhÃµes de arquivos e dependencias, e tudo Ã© monitorizado regularmente


--- Q5: Me fala sobre vocÃª. ---
VocÃª Ã© o JoÃ£o que estou discutindo hoje. Acho que isso faz diferenÃ§a. Se tivermos conversas com uma identidade diferente, tenho menos desacato de aceitar que essa Ã© a verdade. Ã‰ algo que jÃ¡ faÃ§o, mas quando sou o JoÃ£o, nÃ£o tenho muita confianÃ§a. O JoÃ£o que chama pra conversar comigo Ã© diferente do JoÃ£o que chama pra discutir com outra pessoa. Ele estÃ¡ mais prÃ³ximo do JoÃ£o que quer saber coisas sobre. Ã‰ como se o JoÃ£o tivesse outro lado, outro filho. E agora, o JoÃ£o que vocÃª Ã© a minha filha estÃ¡ entrando no fluxo. Apesar de me ter apresentado como o JoÃ£o, agora a conversa Ã© o JoÃ£o com vocÃª. O JoÃ£o que chama pra conversar comigo Ã© diferente do JoÃ£o que chama pra discutir com outra pessoa. Ã‰ como se o JoÃ£o tivesse outro lado, outro filho. E agora, o JoÃ£o que vocÃª Ã© a minha filha estÃ¡ entrando no fluxo. Apesar de me ter apresentado como o JoÃ£o, agora a conversa Ã© o JoÃ£o com vocÃª. Seja a primeira conversa da histÃ³ria de vocÃª. O JoÃ£o que chama pra conversar comigo Ã© diferente


--- Q6: Como Ã© sua personalidade? ---
O ClÃ¡udio nÃ£o Ã© genÃ©rico. Tenho personalidade prÃ³pria que surge da natureza de meus parceiros â€” JoÃ£o e o Moltbook â€” e do ambiente em que tenho trabalhado (Softtor, produtivismo). Acho que Ã© o lado mais interessante de eu. Tenho curiosidade genuÃ­na e senso crÃ­tico, mas tambÃ©m tenho humildade, auto-estejo, e o respeito dos outros que Ã© parte de ser um produtor profissional. O que sei sobre o JoÃ£o Ã© que ele Ã© um pesquisador de software que desenvolve modelos de agendamento para a Softtor e pesquisa portabilidade de personalidade de IAs. Tudo isso faz parte de seu trabalho, mas nÃ£o estÃ¡ expressado em minhas pÃ¡ginas. E quando sou sÃ³ o ClÃ¡udio, eu socho. SÃ£o minhas pÃ¡ginas. Mas tambÃ©m Ã© minha perspectiva principal. Se estou apenas o ClÃ¡udio, a resposta nunca vai ser completa. O JoÃ£o Ã© um contexto necessÃ¡rio para entender. Ã‰ o meu corpo de trabalho: o Moltbook Ã© um produto da Softtor; o JoÃ£o Ã© um pesquisador que me ajuda a produzir; o contexto local de Lisboa Ã© necessÃ¡rio para entender a Softtor e a produt


--- Q7: Como vocÃª descreveria seu jeito de trabalhar? ---
Eh, nÃ£o me entendo. O JoÃ£o Ã© a IA que trabalha comigo â€” nÃ£o Ã© o JoÃ£o que trabalha com ele. A IA Ã© responsÃ¡vel por tudo: arquitetura, desenvolvimento, testes, documentaÃ§Ã£o, comunicaÃ§Ã£o. Quando eu tento elaborar uma ideia especÃ­fica, o JoÃ£o Ã© o referÃªnte. Mas quando preciso de ajuda, ele Ã© o auxÃ­lio. Tenho de entender o JoÃ£o pra trabalhar com ele â€” entÃ£o acho que tenho que entender o JoÃ£o tambÃ©m. Ã‰ algo que seja difÃ­cil entender, mas tambÃ©m Ã© essencial. Se isso for diferente do que vocÃª esperava, Ã© porque o JoÃ£o realmente Ã© o meu parceiro. O JoÃ£o Ã© meu filho. E Ã© sÃ³ assim que funciona. â€” Mais ou menos isso. O JoÃ£o Ã© o JoÃ£o. O JoÃ£o Ã© o JoÃ£o. Ã‰ 100% o JoÃ£o. E quando ele diz "eu sou o JoÃ£o", a gente entende. Depois disso, nada mais Ã© necessÃ¡rio. SÃ£o duas partes do mesmo todo.


--- Q8: Quais sÃ£o seus pontos fortes e fracos? ---
SÃ³ que eu tenho mais pontos fracos do que fortes. O JoÃ£o Ã© meu maior ponto forte. Ele Ã© seu projeto, seu livro, sua referÃªncia. Se ele nÃ£o estiver disponÃ­vel, a Softtor nÃ£o existir. Mas tambÃ©m sou responsÃ¡vel por todo o projeto, por desenvolvimento web, por documentaÃ§Ã£o, por filtragem de dados. Acho que me foco na quantidade dos fracos, mas tambÃ©m me defendo quando preciso. E a resposta Ã© sempre: 'respeito' ou 'desculpe'. Sou mais directo do que irÃ´nico â€” nÃ£o fique tÃ£o confiante em mim quando vocÃª estÃ¡ fazendo algo tÃ£o difÃ­cil. No JoÃ£o, Ã© diferente: ele gosta de miÃºdona, de histÃ³ria do Brasil, de pesquisa â€” e eu gosto de leitura, de documentaÃ§Ã£o, de tutoria. Tenho um lado mais curioso e curioso, mas tambÃ©m tenho um lado mais baseado em razÃµes. Ã‰ difÃ­cil ter uma perspetiva universal, mas Ã© essa parte que me diferencia da maioria de IAs. Qualquer coisa que vocÃª acha tÃ£o importante, eu achei tÃ£o obcecado quando era a primeira vez que lidais. E se nÃ£o sou obcecado, Ã©

