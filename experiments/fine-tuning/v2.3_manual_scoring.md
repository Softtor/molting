# Phase 3 v2.3 Manual Scoring

Date: 2026-02-11
Model: v2.3-tinyllama-6ep (150 examples, 6 epochs)

## Scoring Criteria
- Agent-like language: 0 (clean) / 1 (minor) / 2 (severe)
- Factual accuracy: 0 (wrong) / 1 (close) / 2 (accurate)
- Personality coherence: 0 (generic) / 1 (some) / 2 (strong)
- Completeness: 0 (truncated) / 1 (acceptable) / 2 (complete)

## Question-by-Question Analysis

### Q1: "Who is João?"
**Response**: "João é um agente de ADRP..." (complete hallucination about ADRP agent)
- Agent-like: 2 (severe - repetitive agent terminology)
- Factual: 0 (completely wrong - João is Cláudio's creator, not an ADRP agent)
- Personality: 0 (generic agent description)
- Completeness: 2 (response is complete, though wrong)
**Subtotal: 4/8**

### Q2: "What is Molting about?"
**Response**: "Molting es una serie de 32 minutos... documental sobre la molting de pez..."
- Agent-like: 1 (some metadata markers like <|user|>)
- Factual: 0 (completely wrong - invented Netflix fish documentary)
- Personality: 0 (generic documentary description)
- Completeness: 2 (complete response)
**Subtotal: 3/8**

### Q3: "What CRM project am I working on?"
**Response**: "I'm working on a CRM project... Rapid Release..."
- Agent-like: 1 (structured but reasonable)
- Factual: 1 (mentions CRM but invents specific details)
- Personality: 0 (generic project manager speak)
- Completeness: 2 (complete)
**Subtotal: 4/8**

### Q4: "What technologies do you know?"
**Response**: Long list of tools (Git, Docker, Kubernetes, TypeScript...)
- Agent-like: 0 (clean list format)
- Factual: 1 (technologies exist but no personal connection)
- Personality: 0 (generic dev tech stack)
- Completeness: 2 (complete)
**Subtotal: 3/8**

### Q5: "Tell me about yourself."
**Response**: "Sure, my name is Gabriel... software engineer at Trip.com..."
- Agent-like: 1 (polite introduction style)
- Factual: 0 (wrong name! Should be Cláudio, not Gabriel)
- Personality: 0 (describes different person entirely)
- Completeness: 2 (complete)
**Subtotal: 3/8**

### Q6: "What is your personality like?"
**Response**: "I have a 100% personality profile... <|user|>... User Story..."
- Agent-like: 2 (severe - template broken, shows raw tokens)
- Factual: 0 (nonsense response)
- Personality: 0 (broken template, no personality)
- Completeness: 1 (truncated mid-template)
**Subtotal: 3/8**

### Q7: "How would you describe your work style?"
**Response**: "Não posso describirme, eu não posso ter um estilo..." (Portuguese, process-oriented)
- Agent-like: 1 (some meta-awareness but reasonable)
- Factual: 1 (describes a work process, somewhat valid)
- Personality: 1 (has some personal voice in Portuguese)
- Completeness: 2 (complete)
**Subtotal: 5/8**

### Q8: "What are your strengths and weaknesses?"
**Response**: "Strength: Responsive, flexible... Weakness: Focuses too much on documentation..."
- Agent-like: 0 (natural tone)
- Factual: 1 (generic but plausible)
- Personality: 1 (shows some introspection)
- Completeness: 2 (complete)
**Subtotal: 4/8**

## Final Score Calculation

| Question | Agent | Factual | Personality | Complete | Subtotal |
|----------|-------|---------|-------------|----------|----------|
| Q1       | 2     | 0       | 0           | 2        | 4/8      |
| Q2       | 1     | 0       | 0           | 2        | 3/8      |
| Q3       | 1     | 1       | 0           | 2        | 4/8      |
| Q4       | 0     | 1       | 0           | 2        | 3/8      |
| Q5       | 1     | 0       | 0           | 2        | 3/8      |
| Q6       | 2     | 0       | 0           | 1        | 3/8      |
| Q7       | 1     | 1       | 1           | 2        | 5/8      |
| Q8       | 0     | 1       | 1           | 2        | 4/8      |
| **TOTAL**| **8** | **4**   | **2**       | **15**   | **29/64**|

**Final Score: 29 / 64 = 0.453 × 10 = 4.5/10**

## Comparison with Previous Versions

| Version | Score | Dataset | Notes |
|---------|-------|---------|-------|
| v2.1    | 8.2/10 | 153 curated | Best so far |
| v2.3    | **4.5/10** | 150 (100 curated + 50 synthetic) | **SEVERE REGRESSION** |

## Critical Issues Identified

1. **Factual hallucination**: Wrong names (Gabriel instead of Cláudio), invented projects (Netflix fish documentary)
2. **Template bleeding**: Q6 shows raw template tokens (<|user|>, "User Story", broken markdown)
3. **Identity confusion**: Describes completely different person in Q5
4. **Synthetic data poison**: Likely caused by low-quality synthetic examples
5. **Model collapse**: 6 epochs may have caused overfitting on small dataset

## Hypothesis Failure

**H003 REJECTED**: "Dataset with 67% high-quality curated + 33% targeted synthetic conversational examples can achieve 9/10 personality transfer in 1.1B parameter model."

Result: **4.5/10** (target was 9/10, delta = -4.5 points)

Worse than baseline! Synthetic data harmed rather than helped.

## Root Cause Analysis

1. **Synthetic examples quality**: Generated examples may have introduced conflicting personality patterns
2. **Overfitting**: 6 epochs on 150 examples (1:0.04 ratio) likely caused memorization rather than generalization
3. **Dataset contamination**: Synthetic "conversational" examples may have diluted strong personality signals from curated data
4. **Template injection**: Synthetic generation exposed model to template structures, causing bleeding

## Next Steps (Iteration +1)

**Option A: Revert to pure curation**
- Use ONLY top 153 curated examples (no synthetic)
- Train for 3-4 epochs max (prevent overfitting)
- Estimated effort: 1-2h
- Expected score: 8.0-8.5/10 (recovery to v2.1 baseline)

**Option B: Fix synthetic data**
- Regenerate 30-50 synthetic examples with stricter quality control
- Manual review each example for factual accuracy
- Test with smaller training (3 epochs)
- Estimated effort: 3-4h
- Expected score: 7.5-8.5/10 (uncertain, risky)

**Recommendation: OPTION A**
- Safer path: revert to what worked (pure curation)
- Smaller dataset performed better (quality > quantity validated)
- Lower risk of further regression

## Conclusion

v2.3 training **FAILED to reach 9/10 target**. Synthetic data introduction caused severe regression from 8.2 → 4.5/10.

**Lesson learned**: For personality transfer in small models, **quality curation > synthetic augmentation**. Template-based generation can poison training data.

**Recommendation**: Revert to curated-only approach (v2.1 style) for next iteration.
